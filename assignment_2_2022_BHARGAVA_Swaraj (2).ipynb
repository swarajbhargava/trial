{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "DI8DIAIBdPQ7"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1><center>Big Data Algorithms Techniques & Platforms wtf </center></h1>\n",
        "<h2>\n",
        "<hr style=\" border:none; height:3px;\">\n",
        "<center>Spark and DataFrames</center>\n",
        "<hr style=\" border:none; height:3px;\">\n",
        "</h2>"
      ],
      "metadata": {
        "id": "y0S3PBIZQ2NH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A.  Analysis of the Divine Comedy\n",
        "\n",
        "Suppose that you have a file that contains the text of the Divine Comedy, a really famous ancient poem written in Italian. You can see an excerpt below :"
      ],
      "metadata": {
        "id": "g53MVOhDv7Oe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Nel mezzo del cammin di nostra vita \n",
        "\n",
        "mi ritrovai per una selva oscura,\n",
        "\n",
        "ché la diritta via era smarrita.\n",
        "\n",
        "Ahi quanto a dir qual era è cosa dura, \n",
        "\n",
        "esta selva selvaggia e aspra e forte, \n",
        "\n",
        "che nel pensier rinova la paura! \n",
        "\n",
        "Tant'è amara che poco è più morte; \n",
        "\n",
        "ma per trattar del ben ch'i' vi trovai, \n",
        "\n",
        "dirò de l'altre cose ch'i' v'ho scorte.\n",
        "\n",
        "Io non so ben ridir com'i' v'intrai, \n",
        "\n",
        "tant'era pien di sonno a quel punto\n",
        "\n",
        "che la verace via abbandonai.\" \n",
        " \n"
      ],
      "metadata": {
        "id": "wwiHRqBNv8Tf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can find here one translation for giving you an idea about the content if you do not know the poem."
      ],
      "metadata": {
        "id": "Z8PA1Ghxw804"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Midway upon the journey of our life\n",
        "\n",
        "I found myself within a forest dark,\n",
        "\n",
        "For the straightforward pathway had been lost.\n",
        "\n",
        "Ah me! how hard a thing it is to say\n",
        "\n",
        "What was this forest savage, rough, and stern,\n",
        "\n",
        "Which in the very thought renews the fear. \n",
        "\n",
        "So bitter is it, death is little more;\n",
        "\n",
        "But of the good to treat, which there I found,\n",
        "\n",
        "Speak will I of the other things I saw there.\n",
        "\n",
        "I cannot well repeat how there I entered,\n",
        "\n",
        "So full was I of slumber at the moment\n",
        "\n",
        "In which I had abandoned the true way.\"\n"
      ],
      "metadata": {
        "id": "Zs9P60qcxEBm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <strong>Exercise 0.</strong> Support functions"
      ],
      "metadata": {
        "id": "DI8DIAIBdPQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Write here all the import function and the support function you need for processing the text\n",
        "\n",
        "#Executing PySpark commands\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop2.tgz\n",
        "!tar zxvf spark-3.3.1-bin-hadoop2.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.1-bin-hadoop2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark import SparkConf, SparkContext\n",
        "conf = SparkConf().setMaster(\"local\")\n",
        "sc = SparkContext.getOrCreate(conf = conf)\n",
        "print(\"initialization successful!\")\n",
        "\n",
        "import numpy as np\n",
        "import random as rn\n",
        "\n",
        "seed_value=0\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "\n",
        "#importing the text file for processing\n",
        "\n",
        "with open('divina_commedia.txt','r') as file:\n",
        "    poem = file.read().splitlines()\n",
        "\n",
        "#We get indices of Cantos within Inferno, Purgatorio and Paradiso\n",
        "indices_of_inferno_cantos = {}\n",
        "indices_of_purga_cantos = {}\n",
        "indices_of_paradiso_cantos = {}\n",
        "index_of_end = 0\n",
        "inferno = '  Inferno • Canto'\n",
        "purgatorio = '  Purgatorio • Canto'\n",
        "paradiso = '  Paradiso • Canto'\n",
        "\n",
        "#looping through the poem to create dictionary of indexes.\n",
        "for x in poem:\n",
        "    if inferno in x:\n",
        "        indices_of_inferno_cantos[x] = poem.index(x) \n",
        "    if purgatorio in x:\n",
        "        indices_of_purga_cantos[x] = poem.index(x)\n",
        "    if paradiso in x:\n",
        "        indices_of_paradiso_cantos[x] = poem.index(x)\n",
        "    if '- - - - - - - - - - - - - - - - - - - - - - -' in x:\n",
        "        index_of_end = poem.index(x)\n",
        "\n",
        "poem_b = poem[indices_of_inferno_cantos['  Inferno • Canto I'] : index_of_end]\n",
        "\n",
        "'''Here, we divide the poem into three sections based on the three cantoi - Inferno, Purgatorio, Paradiso.'''\n",
        "\n",
        "#For ex, we get the index by converting the dictionary into list and then taking first index of inferno till first for purgatorio for inferno section\n",
        "\n",
        "inferno_canti = poem[list(indices_of_inferno_cantos.items())[0][1]: list(indices_of_purga_cantos.items())[0][1]]\n",
        "purga_canti = poem[list(indices_of_purga_cantos.items())[0][1]:list(indices_of_paradiso_cantos.items())[0][1]]\n",
        "paradiso_canti = poem[list(indices_of_paradiso_cantos.items())[0][1]:index_of_end]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bRoe2rJxdIvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For exercise 2\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop2.tgz\n",
        "!tar zxvf spark-3.3.1-bin-hadoop2.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.1-bin-hadoop2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "#import of the SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "#inizialization of the Spark Session\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Lab3\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "xf57hPeXa3cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <strong>Exercise 1</strong> Word number in verses (1 point)\n",
        "\n",
        "Write and comment the set of Spark operations that give the average number of words in a verse.\n",
        "\n",
        "A verse in poetry is a line of text\n"
      ],
      "metadata": {
        "id": "Jeq1OBoKc7aS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize(poem_b)\n",
        "#Here, we remove the sentences which are headings of cantos\n",
        "rdd_withoutheadings = rdd.filter(lambda x: x not in indices_of_inferno_cantos)\\\n",
        "                         .filter(lambda x: x not in indices_of_purga_cantos)\\\n",
        "                         .filter(lambda x: x not in indices_of_paradiso_cantos)\n",
        "#rdd.collect()\n",
        "\n",
        "'''Steps followed :\n",
        "1. splitting our list into list of lists of words\n",
        "2. removing those words which are actually spaces\n",
        "3. creating a pair RDD for average purpose'''\n",
        "\n",
        "rdd_q1 = rdd_withoutheadings.map(lambda x: x.split())\\\n",
        "           .filter(lambda x: len(x) > 0)\\\n",
        "           .map(lambda x: ('Sentence',len(x))) \n",
        "           \n",
        "num_of_verses = rdd_q1.count()\n",
        "total_words_in_poem = rdd_q1.reduceByKey(lambda x, y: x + y).collect()[0][1]\n",
        "\n",
        "average_num_of_words_in_verse = total_words_in_poem / num_of_verses\n",
        "print('The average number of words per verse is : ', round(average_num_of_words_in_verse,3))\n"
      ],
      "metadata": {
        "id": "bg6DHjA5daP1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19d33e34-1146-43db-9126-bf69f83537f2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The average number of words per verse is :  6.833\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <strong>Exercise 2.</strong> Punctuation (1 point)\n",
        "\n",
        "Write and comment the set of Spark operations that give the number of verses ending with a punctuation symbol."
      ],
      "metadata": {
        "id": "a9qdDteilCiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import string library function \n",
        "import string \n",
        "result = string.punctuation #this is a string with all punctuations\n",
        "\n",
        "#first filter gets us only those sentences which are empty i.e spaces\n",
        "rdd_q2 = rdd_withoutheadings.filter(lambda x: len(x) > 0)\\\n",
        "                            .filter(lambda x: x[-1] in result)\n",
        "                            \n",
        "verses_ending_in_punctuation = rdd_q2.count()\n",
        "\n",
        "print('The number of verses ending in punctuations are :', verses_ending_in_punctuation)"
      ],
      "metadata": {
        "id": "SpfbBE_KlPe1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7b8db97-3143-4645-d03b-78ec406c6e87"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of verses ending in punctuations are : 9494\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <strong>Exercise 3.</strong>Word number in the text (1 point)\n",
        "\n",
        "Write and comment the set of Spark operations that return the 30 most frequent words in the provided text."
      ],
      "metadata": {
        "id": "Ldd9xwUNdGGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Here I interpret the question as the 30 most frequent words in the WHOLE TEXT FILE'''\n",
        "\n",
        "import operator\n",
        "rdd_text = sc.parallelize(poem)\n",
        "rdd_q3 = rdd_text.filter(lambda x: len(x) > 0)\\\n",
        "                            .flatMap(lambda x: x.split()) \\\n",
        "                            .map(lambda x: x.lower())\n",
        "\n",
        "count_of_words =  rdd_q3.countByValue() #using countByValue and converting to a dictionary to sort\n",
        "\n",
        "sorted_count_of_words = sorted(count_of_words.items(), key=operator.itemgetter(1),reverse=True) #sorting dictionary by value\n",
        "\n",
        "print('30 Most frequent words in the text with their frequencies are : ',sorted_count_of_words[:31])"
      ],
      "metadata": {
        "id": "48F6wdNBkADi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e2b1c54-c370-4696-9fdc-36625067606d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30 Most frequent words in the text with their frequencies are :  [('e', 3994), ('che', 3544), ('la', 2343), ('a', 1892), ('di', 1877), ('non', 1422), ('per', 1365), ('in', 1151), ('si', 1042), ('’l', 958), ('le', 794), ('li', 779), ('sì', 740), ('mi', 736), ('io', 660), ('il', 653), ('più', 645), ('con', 642), ('come', 622), ('da', 619), ('è', 604), ('de', 598), ('lo', 586), ('del', 568), ('al', 505), ('ma', 494), ('tu', 441), ('se', 414), ('ne', 384), ('quel', 367), ('nel', 307)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Another way to do it is interpretation where we remove stopwords of both italian and english from the most frequent words'''\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "e_stopwords = stopwords.words('english')\n",
        "i_stopwords = stopwords.words('italian')\n",
        "\n",
        "i_stopwords\n",
        "\n",
        "\n",
        "import operator\n",
        "rdd_text = sc.parallelize(poem)\n",
        "rdd_q3 = rdd_text.filter(lambda x: len(x) > 0)\\\n",
        "                            .flatMap(lambda x: x.split()) \\\n",
        "                            .map(lambda x: x.lower()) \\\n",
        "                            .filter(lambda x: x not in e_stopwords)\\\n",
        "                            .filter(lambda x: x not in i_stopwords) \n",
        "                            \n",
        "\n",
        "count_of_words =  rdd_q3.countByValue() #using countByValue and converting to a dictionary to sort\n",
        "\n",
        "sorted_count_of_words = sorted(count_of_words.items(), key=operator.itemgetter(1),reverse=True) #sorting dictionary by value\n",
        "\n",
        "print('30 Most frequent words in the text with their frequencies are : ',sorted_count_of_words[:31])"
      ],
      "metadata": {
        "id": "gY7gED5wKVyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <strong>Exercise 4.</strong> Beginning word (1 point)\n",
        "\n",
        "Write and comment the set of Spark operations that return the most frequent word at the beginning of a verse and the number of times they appear."
      ],
      "metadata": {
        "id": "eblGspzMkBNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_q4 = rdd_withoutheadings.filter(lambda x: len(x) > 0)\\\n",
        "                            .map(lambda x: x.split()) \\\n",
        "                            .map(lambda x: x[0])\n",
        "                          \n",
        "frequency_of_first_words = rdd_q4.countByValue() #a dictionary with first words and corresponding frequencies\n",
        "\n",
        "#printing the most frequent word at the beginning of a verse\n",
        "most_freq_fword = max(frequency_of_first_words.items() , key= lambda k: k[1])\n",
        "print('The most frequent first word is :' , most_freq_fword[0], 'and it appears', most_freq_fword[1], 'times.' )\n"
      ],
      "metadata": {
        "id": "G58B9ErwkGx0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b284596-1b25-4a56-cbb7-40a335394a31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most frequent first word is : e and it appears 1056 times.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##<strong>Exercise 5.</strong>  Ending Word (1 point)\n",
        "\n",
        "Write and comment the set of Spark operations that return the 20 most frequent word at the end of a verse and the number of times they appear."
      ],
      "metadata": {
        "id": "SIfv-3-LkIUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_q5 = rdd_withoutheadings.filter(lambda x: len(x) > 0)\\\n",
        "                            .map(lambda x: x.split()) \\\n",
        "                            .map(lambda x: x[-1])\n",
        "                          \n",
        "frequency_of_last_words = rdd_q5.countByValue() #a dictionary with first words and corresponding frequencies\n",
        "\n",
        "#printing the most frequent word at the beginning of a verse\n",
        "\n",
        "sorted_most_freq_lword = sorted(frequency_of_last_words.items(), key=operator.itemgetter(1),reverse=True)\n",
        "print('The 20 most frequent last words are :' , sorted_most_freq_lword[:21] )"
      ],
      "metadata": {
        "id": "Iwfx6rR-kQAn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "230d2bea-ddf8-46a5-89f7-fdd6f8179fca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 20 most frequent last words are : [('parte', 16), ('poco,', 14), ('segno', 13), ('loco', 13), ('parte,', 12), ('mio,', 11), ('riva', 11), ('disio', 11), ('mondo,', 11), ('vede,', 11), ('quella', 11), ('fui,', 10), ('cose', 10), ('puote', 10), ('vita,', 10), ('forte,', 10), ('prima', 10), ('mente', 10), ('fue', 10), ('natura', 10), ('virtute', 10)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##<strong>Exercise 6.</strong> Ending of consecutive verses (2 points)\n",
        "\n",
        "Write and comment the set of Spark operations that returns the most frequent pairs of words at the end of two consecutive verses.\n",
        "\n",
        "Tant'è amara che poco è più morte; \n",
        "\n",
        "ma per trattar del ben ch'i' vi trovai, \n",
        "\n",
        "dirò de l'altre cose ch'i' v'ho scorte.\n",
        "\n",
        "For these 3 verses we have as pairs (morte, trovai), (trovai, scorte)."
      ],
      "metadata": {
        "id": "G0Q4AKO4kW3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The last line removes punctuations from the word\n",
        "rdd_q6 = rdd_withoutheadings.filter(lambda x: len(x) > 0) \\\n",
        "                            .map(lambda x: x.split()) \\\n",
        "                            .map(lambda x: x[-1]) \\\n",
        "                            .map(lambda x: x.translate(str.maketrans('','', string.punctuation)))\n",
        "rdd_new = rdd_q6.zipWithIndex()\n",
        "#we now create two rdds, use the index as key, join them, and then drop the index\n",
        "original_rdd = rdd_new.map(lambda x: (x[1], x[0]))\n",
        "shifted_rdd = rdd_new.map(lambda x: (x[1]-1, x[0]))\n",
        "results = original_rdd.join(shifted_rdd)\n",
        "abc = results.values()\n",
        "\n",
        "#Now we can apply reducebyKey and get the number of repetitions\n",
        "result = abc.map(lambda x: (x,1))\\\n",
        "          .reduceByKey(lambda x,y: x+y)\\\n",
        "          .sortBy(lambda f: f[1], ascending=False)\n",
        "\n",
        "print('The top 10 most repeating pairs are :')\n",
        "result.take(10)"
      ],
      "metadata": {
        "id": "9d2_nmSMl8rd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cf8e817-51e5-436b-9d22-484867af7899"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The top 10 most repeating pairs are :\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('puote', 'dimandare»'), 2),\n",
              " (('messo', 'segno'), 2),\n",
              " (('mio', 'fatto'), 2),\n",
              " (('quetarsi', 'move'), 2),\n",
              " (('segno', 'esso'), 2),\n",
              " (('passo', 'viva'), 2),\n",
              " (('pace', 'poco'), 2),\n",
              " (('lasso', 'messo'), 2),\n",
              " (('scocca', 'l’arco'), 2),\n",
              " (('infuso', 'fece'), 2)]"
            ]
          },
          "metadata": {},
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <strong>Exercise 7.</strong> Most common  words in the poem (1 point)\n",
        "\n",
        "Write and comment the set of Spark operations that returns the 30 most common word in the poem (a poem is a set of verses).\n",
        "\n"
      ],
      "metadata": {
        "id": "OQMMojiUlZLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Without removing italian stopwords'''\n",
        "rdd_q7 = rdd_withoutheadings.filter(lambda x: len(x) > 0)\\\n",
        "                            .flatMap(lambda x: x.split())                 \n",
        "\n",
        "count_of_words =  rdd_q7.countByValue() #using countByValue and converting to a dictionary to sort\n",
        "\n",
        "sorted_count_of_words = sorted(count_of_words.items(), key=operator.itemgetter(1),reverse=True) #sorting dictionary by value\n",
        "\n",
        "print('Most frequent words in the poem with their frequencies are : ',sorted_count_of_words[:31])\n",
        "\n"
      ],
      "metadata": {
        "id": "k7XQqVoyl6r0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb951951-3edd-4478-9316-6aa0ec67dd17"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most frequent words in the poem with their frequencies are :  [('e', 3613), ('che', 3535), ('la', 2261), ('di', 1822), ('a', 1805), ('non', 1338), ('per', 1319), ('in', 1071), ('si', 1042), ('’l', 958), ('le', 777), ('li', 753), ('mi', 736), ('sì', 706), ('il', 652), ('più', 637), ('con', 623), ('è', 600), ('de', 593), ('da', 593), ('come', 565), ('del', 564), ('io', 544), ('lo', 521), ('al', 498), ('tu', 405), ('E', 376), ('se', 376), ('ne', 375), ('ma', 353), ('quel', 349)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''With removing italian stopwords'''\n",
        "rdd_q7 = rdd_withoutheadings.filter(lambda x: len(x) > 0)\\\n",
        "                            .flatMap(lambda x: x.split()) \\\n",
        "                            .map(lambda x: x.lower())\\\n",
        "                            .filter(lambda x: x not in i_stopwords)                \n",
        "\n",
        "count_of_words =  rdd_q7.countByValue() #using countByValue and converting to a dictionary to sort\n",
        "\n",
        "sorted_count_of_words = sorted(count_of_words.items(), key=operator.itemgetter(1),reverse=True) #sorting dictionary by value\n",
        "\n",
        "print('Most frequent words in the poem with their frequencies are : ',sorted_count_of_words[:31])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ai7LUo68MqrB",
        "outputId": "47263829-d5f4-4a5e-af13-ecc90627d9ff"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most frequent words in the poem with their frequencies are :  [('’l', 958), ('sì', 740), ('de', 598), ('quel', 367), ('così', 284), ('poi', 276), ('quando', 260), ('già', 255), ('là', 251), ('son', 227), ('tanto', 223), ('ch’io', 223), ('me', 222), ('qual', 201), ('occhi', 199), ('ben', 194), ('lor', 191), ('sé', 185), ('ché', 178), ('qui', 168), ('né', 167), ('com’', 165), ('fa', 161), ('pur', 159), ('ciò', 157), ('vidi', 151), ('però', 143), ('giù', 141), ('ch’i’', 136), ('prima', 135), ('tal', 133)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <strong>Exercise 8.</strong> Most common words by \"Canto\" (1 point)\n",
        "\n",
        "The Divina Commedia is divided in 3 Canti: Inferno, Purgatorio, Paradiso.\n",
        "\n",
        "Write and comment the set of Spark operations that returns the most common 30 words for each \"Canto\"."
      ],
      "metadata": {
        "id": "nBDAbUwulkOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cantis = [(inferno_canti,'Inferno'), (purga_canti,'Purgatorio'), (paradiso_canti,'Paradiso')]\n",
        "for canto,name in cantis:\n",
        "    rdd_dummy = sc.parallelize(canto)\n",
        "    rdd_q8 = rdd_dummy.filter(lambda x: len(x) > 0)\\\n",
        "                            .flatMap(lambda x: x.split())\n",
        "    count_of_words =  rdd_q8.countByValue() #using countByValue and converting to a dictionary to sort\n",
        "\n",
        "    sorted_count_of_words = sorted(count_of_words.items(), key=operator.itemgetter(1),reverse=True) #sorting dictionary by value\n",
        "\n",
        "    print('30 most common words in',name ,'are:',sorted_count_of_words[:31])\n",
        "\n"
      ],
      "metadata": {
        "id": "3M_OcH0ul98L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0d1f2fd-f833-442a-b172-97505489dfa7"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30 most common words in Inferno are: [('e', 1215), ('che', 1121), ('la', 751), ('a', 609), ('di', 554), ('non', 452), ('’l', 420), ('per', 415), ('in', 292), ('le', 290), ('mi', 288), ('li', 288), ('si', 285), ('con', 246), ('sì', 246), ('io', 212), ('tu', 195), ('de', 194), ('è', 190), ('il', 183), ('lo', 182), ('da', 175), ('più', 173), ('al', 171), ('del', 154), ('E', 154), ('come', 154), ('ne', 119), ('se', 119), ('un', 118), ('quel', 114)]\n",
            "30 most common words in Purgatorio are: [('e', 1206), ('che', 1159), ('la', 712), ('di', 629), ('a', 604), ('non', 473), ('per', 458), ('in', 365), ('si', 329), ('le', 280), ('’l', 279), ('mi', 246), ('più', 229), ('li', 226), ('il', 218), ('da', 210), ('sì', 200), ('è', 200), ('con', 195), ('come', 189), ('io', 183), ('de', 179), ('del', 173), ('lo', 166), ('al', 163), ('ne', 131), ('se', 130), ('ma', 126), ('E', 111), ('ti', 106), ('tu', 105)]\n",
            "30 most common words in Paradiso are: [('che', 1255), ('e', 1192), ('la', 798), ('di', 639), ('a', 592), ('per', 446), ('si', 428), ('in', 414), ('non', 413), ('sì', 260), ('’l', 259), ('il', 251), ('li', 239), ('del', 237), ('più', 235), ('come', 222), ('de', 220), ('è', 210), ('da', 208), ('le', 207), ('mi', 202), ('con', 182), ('lo', 173), ('al', 164), ('quel', 152), ('io', 149), ('suo', 133), ('nel', 133), ('se', 127), ('ne', 125), ('sua', 124)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# B. Tripadvisor European Restaurant \n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "For running this series of exercises, we are going to use a dataset coming from <a href=\"https://www.kaggle.com/datasets/stefanoleone992/tripadvisor-european-restaurants\">Kaggle</a>.\n",
        "\n",
        "As stated in the description of the dataset:\n",
        "\"The TripAdvisor dataset includes 1,083,397 restaurants with attributes such as location data, average rating, number of reviews, open hours, cuisine types, awards, etc.\n",
        "\n",
        "The dataset combines the restaurants from the main European countries\".\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "\n",
        "### The dataset\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "The dataset is in a .csv file, and among the columns, you can find:\n",
        "\n",
        "<ul>\n",
        "    <li> <code>Restaurant_link</code>\tthe link of the restaurant in TripAdvisor </li>\n",
        "    <li><code>Restaurant_name</code>\tThe name of the restaurant</li>\n",
        "    <li><code>Original_location</code>\tThe location of the restaurant</li>\n",
        "    <li><code>Country</code>\tThe country</li>\n",
        "    <li><code>...</code>\t...</li>\n",
        "</ul>\n",
        "</p>\n",
        "</font>"
      ],
      "metadata": {
        "id": "5a0nrGmRQ7Yk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install kaggle\n",
        "\n",
        "! mkdir ~/.kaggle\n",
        "\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "i-FdlIK_9tT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip tripadvisor_european_restaurants.csv.zip"
      ],
      "metadata": {
        "id": "MJ-P2NQd9uKI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "428053f8-2158-4c1b-f355-502ebbb91f6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  tripadvisor_european_restaurants.csv.zip\n",
            "  inflating: tripadvisor_european_restaurants.csv  \n",
            "  inflating: __MACOSX/._tripadvisor_european_restaurants.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <strong> Spark and Pandas.</strong>\n",
        "\n",
        "For this set of exercises you must import data in Spark. After this first import you can pass any dataset to Pandas for the data analysis. At the end of each exercise (when the question is pertinent) you must return (reconvert) the dataframe in Spark.\n",
        "\n",
        "Each time you do this conversion you must comment about this. Example:\n",
        "\n",
        "<code> # creating a Spark dataframe </code>\n",
        "\n",
        "<code> df = ... </code>\n",
        "\n",
        "<code> # using Pandas and creating a Pandas dataframe </code>\n",
        "\n",
        "<code> dfp = df. ... </code>\n",
        "\n",
        "<code> # back to Spark </code>\n",
        "\n",
        "<code> dfs =  ... </code>"
      ],
      "metadata": {
        "id": "zr5auOga9X0Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <strong> Exercise 9.</strong> First import (1 point)\n",
        "Import the csv file in Spark DataFrame and show the structure of the dataframe."
      ],
      "metadata": {
        "id": "WBNt1hUkTGUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the command that creates (reads) a Spark DataFrame and stores the reference in the dfs variable\n",
        "\n",
        "path=\"/content/drive/MyDrive/tripadvisor_european_restaurants.csv\"\n",
        "dfs=spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").option(\"escape\", \"\\\"\").load(path)\n",
        "\n",
        "# show the DataFrame schema\n",
        "dfs.count"
      ],
      "metadata": {
        "id": "bM5Oi6vMR5OC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aed3533a-f903-445a-da81-7ace28d5a502"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method DataFrame.count of DataFrame[restaurant_link: string, restaurant_name: string, original_location: string, country: string, region: string, province: string, city: string, address: string, latitude: string, longitude: string, claimed: string, awards: string, popularity_detailed: string, popularity_generic: string, top_tags: string, price_level: string, price_range: string, meals: string, cuisines: string, special_diets: string, features: string, vegetarian_friendly: string, vegan_options: string, gluten_free: string, original_open_hours: string, open_days_per_week: string, open_hours_per_week: string, working_shifts_per_week: double, avg_rating: double, total_reviews_count: double, default_language: string, reviews_count_in_default_language: double, excellent: string, very_good: double, average: double, poor: double, terrible: double, food: double, service: double, value: double, atmosphere: double, keywords: string]>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOrj5SJOpEKm",
        "outputId": "40a71823-a026-4cc5-fc4e-c4624b435b71"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <strong> Exercise 10.</strong> Data type (1 point)\n",
        "Check and comment about the data type of each column. As you know a good data analysis always starts from a good understanding of your dataset.\n"
      ],
      "metadata": {
        "id": "BnUfWlFkhq-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing the data type of each column\n",
        "dfs.printSchema()\n",
        "'''We see that majority of the columns are strings while the numeric columns are doubles or floats. \n",
        "We can easily process these datatypes as our requirements going forward.'''"
      ],
      "metadata": {
        "id": "Z84CPDOvh6Tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <strong> Exercise 11.</strong> Null (1 point)\n",
        "Show how many null values you have in each column."
      ],
      "metadata": {
        "id": "Jp_hWlx_9DMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Find Count of Null, None, NaN of All DataFrame Columns\n",
        "from pyspark.sql.functions import col,isnan, when, count\n",
        "\n",
        "num_of_nulls = dfs.select([count(when(col(c).isNull(), c)).alias(c) for c in dfs.columns])\n",
        "print('Below dataframe shows count of null values in each column')\n",
        "num_of_nulls.show()"
      ],
      "metadata": {
        "id": "H1UDup1G-ka_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11856afc-1452-420e-9a6f-a2e4f3c031b5"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below dataframe shows column wise count of null values in each columns\n",
            "+---------------+---------------+-----------------+-------+------+--------+------+-------+--------+---------+-------+------+-------------------+------------------+--------+-----------+-----------+------+--------+-------------+--------+-------------------+-------------+-----------+-------------------+------------------+-------------------+-----------------------+----------+-------------------+----------------+---------------------------------+---------+---------+-------+-----+--------+------+-------+------+----------+--------+\n",
            "|restaurant_link|restaurant_name|original_location|country|region|province|  city|address|latitude|longitude|claimed|awards|popularity_detailed|popularity_generic|top_tags|price_level|price_range| meals|cuisines|special_diets|features|vegetarian_friendly|vegan_options|gluten_free|original_open_hours|open_days_per_week|open_hours_per_week|working_shifts_per_week|avg_rating|total_reviews_count|default_language|reviews_count_in_default_language|excellent|very_good|average| poor|terrible|  food|service| value|atmosphere|keywords|\n",
            "+---------------+---------------+-----------------+-------+------+--------+------+-------+--------+---------+-------+------+-------------------+------------------+--------+-----------+-----------+------+--------+-------------+--------+-------------------+-------------+-----------+-------------------+------------------+-------------------+-----------------------+----------+-------------------+----------------+---------------------------------+---------+---------+-------+-----+--------+------+-------+------+----------+--------+\n",
            "|              0|              4|                5|      5| 50328|  340641|400694|      8|   15800|    15804|   1857|820273|              94996|             97804|  110649|     277211|     779074|448055|  169118|       743151|  765999|                 15|           15|         12|             489573|            489573|             489573|                 489573|     96646|              52246|           95203|                            95204|    95205|    95205|  95205|95205|   95205|484082| 479120|480715|    821621|  984208|\n",
            "+---------------+---------------+-----------------+-------+------+--------+------+-------+--------+---------+-------+------+-------------------+------------------+--------+-----------+-----------+------+--------+-------------+--------+-------------------+-------------+-----------+-------------------+------------------+-------------------+-----------------------+----------+-------------------+----------------+---------------------------------+---------+---------+-------+-----+--------+------+-------+------+----------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <strong> Exercise 12.</strong>  Gourmandise (1 point)\n",
        "    \n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "We want to find the City with the greatest number of restaurant.\n",
        "    <ul>\n",
        "     <li> the City name </li>\n",
        "     <li> the number of restaurant </li>\n",
        "</ul>\n",
        "You can pass by Pandas but you must return a Spark dataframe\n",
        "</font>\n",
        "</p>"
      ],
      "metadata": {
        "id": "VZ1400g4h7fK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We groupby the spark dataframe by city and then order it in descending to show cities with most restaurants\n",
        "dfs_citywise = dfs.groupBy('city').count()\n",
        "dfs_citywise = dfs_citywise.orderBy('count', ascending=False).dropna()\n",
        "dfs_citywise.show()"
      ],
      "metadata": {
        "id": "I80ZGgPGtUd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <strong> Exercise 13.</strong>  Most common cuisine in the restaurants by country (2 points)\n",
        "    \n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "We want to explore the most common types (5 types) of Cuisine in each country.\n",
        "</font>\n",
        "</p>"
      ],
      "metadata": {
        "id": "ET9gY8MH21Ew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We convert only the two country and cuisines columns to pandas as toPandas() is an expensive function\n",
        "dfp_q13 = dfs.groupBy('country','cuisines').count().toPandas()\n",
        "dfp_q13 = dfp_q13.fillna('Unknown') #removing nans"
      ],
      "metadata": {
        "id": "QdVft7Fq3pPe"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#processing the cuisines column which contains lists of cuisines per restaurants \n",
        "dfp_cuisines = dfp_q13['cuisines']\n",
        "#we will now convert each row of cuisines column into a list of strings separated by a comma i.e abc\n",
        "abc = []\n",
        "for bunch in dfp_cuisines:\n",
        "  if ',' in bunch:\n",
        "    bunch = bunch.split(',')\n",
        "    abc.append(bunch)\n",
        "  else:\n",
        "    abc.append([bunch])\n",
        "\n",
        "dfp_q13['cuisines_list'] = abc\n",
        "dfp_q13_a = dfp_q13[['country', 'cuisines_list', 'count']]\n",
        "dfp_q13_a.columns = ['Country', 'Cuisine', 'Count']"
      ],
      "metadata": {
        "id": "kPoJTxzDB-BT"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We now use the explode() function to have every row have the same cuisine. We will then use groupby again!\n",
        "exp = dfp_q13_a.explode('Cuisine')\n",
        "exp['Cuisine'] = exp['Cuisine'].str.replace(\" \", \"\")\n",
        "ab = exp.groupby('Country', as_index=False)\n",
        "\n",
        "#dicto is a dictionary of groupby items made countrywise\n",
        "\n",
        "dicto = {}\n",
        "\n",
        "for x in ab.groups:\n",
        "  dicto[x]= ab.get_group(x)\n",
        "  \n",
        "for keys in dicto:\n",
        "  if keys is not float and \"-\" not in keys and \".\" not in keys and keys != 'Unknown': #this is to exclude noisy country names\n",
        "    bsdk = dicto[keys].groupby('Cuisine',as_index=False).count()\n",
        "    bsdk['Cuisine'] = bsdk['Cuisine'].str.replace(' ', '') #removing whitespaces in strings\n",
        "    bsdk.drop('Country', inplace=True, axis=1) #dropping the country column as its not needed\n",
        "    bsdk = bsdk.sort_values('Count', ascending = False).reset_index(drop=True) #descending ordering\n",
        "    bsdk_spark = spark.createDataFrame(bsdk) #convert back to spark dataframe as required\n",
        "    print('In the country', keys, 'the top 5 cuisines are')\n",
        "    bsdk_spark.show(5)\n"
      ],
      "metadata": {
        "id": "2eiJrNzLHfFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <strong> Exercise 14.</strong>  Meal by country (2 points)\n",
        "    \n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "We want to find for which meal there is the greatest offer in each country. \n",
        "</font>\n",
        "</p>"
      ],
      "metadata": {
        "id": "2e-mami838aZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We convert only the two country and meals columns to pandas as toPandas() is an expensive function\n",
        "dfp_q14 = dfs.groupBy('country','meals').count().toPandas()\n",
        "dfp_q14 = dfp_q14.fillna('Unknown') #removing nans"
      ],
      "metadata": {
        "id": "1dr4IFMpvwji"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We use very similar code to previous question here\n",
        "dfp_meals = dfp_q14['meals']\n",
        "\n",
        "abcd = []\n",
        "\n",
        "for bunch in dfp_meals:\n",
        "  if ',' in bunch:\n",
        "    bunch = bunch.split(',')\n",
        "    abcd.append(bunch)\n",
        "  else:\n",
        "    abcd.append([bunch])\n",
        "\n",
        "dfp_q14['meals'] = abcd\n",
        "dfp_q14_a = dfp_q14[['country', 'meals', 'count']]\n",
        "dfp_q14_a.columns = ['Country', 'Meals', 'Count']"
      ],
      "metadata": {
        "id": "hQrl3zGUCX5_"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exp = dfp_q14_a.explode('Meals')\n",
        "exp['Meals'] = exp['Meals'].str.replace(\" \", \"\")\n",
        "xy = exp.groupby('Country', as_index=False)\n",
        "\n",
        "dicto_m = {}\n",
        "for x in xy.groups:\n",
        "  dicto_m[x]= xy.get_group(x)\n",
        "  \n",
        "for keys in dicto_m:\n",
        "  if keys is not float and \"-\" not in keys and \".\" not in keys and keys != 'Unknown':\n",
        "    bsdk = dicto_m[keys].groupby('Meals',as_index=False).count()\n",
        "    bsdk['Meals'] = bsdk['Meals'].str.replace(' ', '')\n",
        "    bsdk.drop('Country', inplace=True, axis=1) #drop the country column\n",
        "    bsdk = bsdk.sort_values('Count', ascending = False).reset_index(drop=True)\n",
        "    bsdk_spark = spark.createDataFrame(bsdk)\n",
        "    print('In the country', keys, 'the top 5 meals are \\n')\n",
        "    bsdk_spark.show(5)\n"
      ],
      "metadata": {
        "id": "68a38QpWvbqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <strong> Exercise 15.</strong>  Meal by country/city (3 points)\n",
        "    \n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "The country and the city in which there are the best-rated restaurants.\n",
        "\n",
        "We want to explore 1) the best-rated restaurant at all and the best-rated restaurant weighted somehow (and you must propose a \"somehow you like\"). \n",
        "\n",
        "For example suppose that in a city there is only one restaurant. How do you consider your best rating?\n",
        "</font>\n",
        "</p>"
      ],
      "metadata": {
        "id": "NWyhAOgxvdp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfp_q15 = dfs.select('restaurant_name','country','city','avg_rating','total_reviews_count').toPandas()"
      ],
      "metadata": {
        "id": "zcdMj2NlwOZl"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating and cleaning dataframe\n",
        "dfp_q15['restaurant_name'] = dfp_q15['restaurant_name'].fillna('Unknown').replace(r\"^ +| +$\", r\"\", regex=True) #removing nans\n",
        "dfp_q15['city'] = dfp_q15['city'].fillna('Unknown').replace(r\"^ +| +$\", r\"\", regex=True) #removing nans\n",
        "dfp_q15['country'] = dfp_q15['country'].fillna('Unknown').replace(r\"^ +| +$\", r\"\", regex=True) #removing nans\n",
        "dfp_q15['avg_rating'] = dfp_q15['avg_rating'].fillna(0)\n",
        "dfp_q15['avg_rating'] = dfp_q15['avg_rating'].astype(float)\n",
        "dfp_q15['total_reviews_count'] = dfp_q15['total_reviews_count'].fillna(0)\n",
        "dfp_q15['total_reviews_count'] = dfp_q15['total_reviews_count'].astype(float)"
      ],
      "metadata": {
        "id": "QBkbLIN7FfWn"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to see top 5 restaurants in all countries along with their ratings\n",
        "#We have weighted them by in descending order of number of reviews\n",
        "xyz = dfp_q15.groupby('country', as_index=False)\n",
        "\n",
        "dicto_c = {}\n",
        "for x in xyz.groups:\n",
        "  dicto_c[x]= xyz.get_group(x)\n",
        "\n",
        "\n",
        "for keys in dicto_c:\n",
        "  if keys is not float and \"-\" not in keys and \".\" not in keys and keys != 'Unknown':\n",
        "    bsdk = dicto_c[keys]\n",
        "    bsdk.drop('country', inplace=True, axis=1) #drop the country column\n",
        "    bsdk = bsdk.sort_values(['avg_rating','total_reviews_count'], ascending = False).reset_index(drop=True)\n",
        "    bsdk_spark = spark.createDataFrame(bsdk)\n",
        "    print('In ', keys, 'the top 5 restaurants are: \\n')\n",
        "    bsdk_spark.show(5)"
      ],
      "metadata": {
        "id": "UisahtFgMBOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Citywise Analysis of Best Restaurants\n",
        "xyza = dfp_q15.groupby('city', as_index=False)\n",
        "\n",
        "dicto_d = {}\n",
        "for x in xyza.groups:\n",
        "  dicto_d[x]= xyza.get_group(x)\n",
        "\n",
        "dict_with_citywise_rest = {}\n",
        "\n",
        "for keys in dicto_d:\n",
        "  if keys is not float and \"-\" not in keys and \".\" not in keys and keys != 'Unknown':\n",
        "    bsdk = dicto_d[keys]\n",
        "    bsdk.drop(['city','country'], inplace=True, axis=1) #drop the country and city column\n",
        "    bsdk = bsdk.sort_values(['avg_rating','total_reviews_count'], ascending = False).reset_index(drop=True)\n",
        "    dict_with_citywise_rest[keys] = bsdk.head(5)\n"
      ],
      "metadata": {
        "id": "7nqH3avLX3GN"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to see the best restaurants in a city \n",
        "def best_5_rest_in_city(city_name):\n",
        "  if city_name in dicto_d:\n",
        "    print(city_name)\n",
        "    mdrchd = spark.createDataFrame(dict_with_citywise_rest[city_name].head(5))\n",
        "    return mdrchd.show(5)\n",
        "  else:\n",
        "    print('Please enter city name as it appears in the dataset')\n",
        "best_rest = best_5_rest_in_city('Palaiseau')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBceYE8IIx-5",
        "outputId": "487f8c94-1aba-4033-bf91-16a8373b4242"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palaiseau\n",
            "+--------------------+----------+-------------------+\n",
            "|     restaurant_name|avg_rating|total_reviews_count|\n",
            "+--------------------+----------+-------------------+\n",
            "|        In Vino Vivo|       5.0|               18.0|\n",
            "|         La Fee Cafe|       5.0|               16.0|\n",
            "|     La Table du 107|       5.0|               14.0|\n",
            "|Le Chichrane - Ch...|       5.0|                1.0|\n",
            "|     Cafe De La Gare|       5.0|                1.0|\n",
            "+--------------------+----------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <strong> Bonus.</strong>  Propose your own analysis\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Propose here an analysis on the dataframe.\n",
        "</font>\n",
        "</p>"
      ],
      "metadata": {
        "id": "2Wpmj23nwTDk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rb-wdPJEwcLF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
